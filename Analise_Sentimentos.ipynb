{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661c0bda-29b9-442c-96d1-4fafbf9daca4",
   "metadata": {},
   "source": [
    "# Análise de Sentimento em Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5183a98-e877-4bca-a333-bac3db61a4ac",
   "metadata": {},
   "source": [
    "#### Importações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351c1f74-1093-47c7-8b82-c5a4e04a2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de975e5b-b996-4855-bfeb-dfd70e0ef188",
   "metadata": {},
   "source": [
    "#### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e5877e-98ef-4865-9eb6-c0934a3c5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps():\n",
    "    print('\\n\\n' + '#'*80+ '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc857d-3454-43cb-b351-c02e7f857318",
   "metadata": {},
   "source": [
    "#### Coletando o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24edf7a5-66f6-4a44-a0db-ad6e5a7d5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sentimentos_tweet.csv\"\n",
    "df = pd.read_csv(path, header=None, encoding='ISO-8859-1', names=['target','id','date','flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57d66f4-cb3f-45ed-b7d7-5787f70177bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=200000)\n",
    "df = df.drop([\"id\", \"date\", \"flag\", \"user\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e19cc2-332f-4b15-b1a2-2d28acddb8c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1313205</th>\n",
       "      <td>4</td>\n",
       "      <td>Band recommendation, I Don't Want To Die In Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028882</th>\n",
       "      <td>4</td>\n",
       "      <td>Aaaand only SEVEN days until Origin 1. HECK YES!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903079</th>\n",
       "      <td>4</td>\n",
       "      <td>FINALLY FINISHED MY FUCKING FILM FINAL!!!  (We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656881</th>\n",
       "      <td>0</td>\n",
       "      <td>tired!! I want to sleep more but I have to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255348</th>\n",
       "      <td>0</td>\n",
       "      <td>Wishing I had money to take my girlfriend out ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "1313205       4  Band recommendation, I Don't Want To Die In Te...\n",
       "1028882       4  Aaaand only SEVEN days until Origin 1. HECK YES! \n",
       "903079        4  FINALLY FINISHED MY FUCKING FILM FINAL!!!  (We...\n",
       "656881        0  tired!! I want to sleep more but I have to wor...\n",
       "255348        0  Wishing I had money to take my girlfriend out ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##############################################################\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 1313205 to 1288408\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   target  200000 non-null  int64 \n",
      " 1   text    200000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.6+ MB\n",
      "None\n",
      "\n",
      "\n",
      "##############################################################\n",
      "\n",
      "\n",
      "Quantidade de valores atribuídos aos sentimentos:\n",
      " 0    100212\n",
      "4     99788\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "display(df.head())\n",
    "ps()\n",
    "print(df.info())\n",
    "ps()\n",
    "print('Quantidade de valores atribuídos aos sentimentos:\\n',df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccab9c75-968a-4262-9bb3-67291a0c8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') # Download das palavras a serem removidas (\"a\", \"o\", \"em\", \"para\")\n",
    "#nltk.download('punkt') # Download dis modelos treinados para tokenização de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62f96b-01d1-4ee9-9761-d10ab18d43a6",
   "metadata": {},
   "source": [
    "#### Preparar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af3ac92-c978-42c7-92a6-b6eeaf8374fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remover caracteres especiais e pontuações\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Converter o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenização do texto (cria um array contendo cada palavra separadamente)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    stop_words = set(stopwords.words('english')) # Carrega o conjunto de palavras consideradas stopwords em inglês\n",
    "    words = [word for word in words if word not in stop_words] # Remove o stopwords da lista\n",
    "    \n",
    "    # Stemming - Obter a palavra na forma raiz\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Reunir as palavras novamente em um texto\n",
    "    processed_text = ' '.join(words)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Aplicar a fuunção de pré-processamento aos textos\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07cabb5-7594-4d2d-9c4a-136edd9871d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4ddf0a-e019-487a-894d-84c2c42f0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz de frequncia de palavras (5000 palavras)\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['processed_text']).toarray()\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb013c2-858b-44e9-8938-81d151d13015",
   "metadata": {},
   "source": [
    "#### Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da105dee-7d88-4545-a3ca-828c68102b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4cf2af-f10f-4795-b396-7a67e15d8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a576945f-6958-4c71-8d85-fb8bdcb7ebf1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68f5fa2-03b4-41b7-a82a-720586bd0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05701ef5-1d9a-4221-82c8-be399a694f16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 76.28%\n",
      "Matriz de confusão\n",
      "[[14796  5352]\n",
      " [ 4136 15716]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Acurácia: {accuracy*100:.2f}%')\n",
    "print('Matriz de confusão')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ee9a76-3059-4ab2-9bf0-7576397e6b55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seu comentário (I love u) é positivo! :D\n"
     ]
    }
   ],
   "source": [
    "new_text = \"I love u\"\n",
    "new_text_processed = preprocess_text(new_text)\n",
    "new_text_processed = vectorizer.transform([new_text]).toarray()\n",
    "prediction = model.predict(new_text_processed)\n",
    "if prediction == 4:\n",
    "    print(f'Seu comentário ({new_text}) é positivo! :D')\n",
    "elif prediction == 0:\n",
    "    print(f'Seu comentário ({new_text}) é negativo! :(')\n",
    "else:\n",
    "    print('Predição fora do padrão')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
